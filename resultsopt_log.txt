{"attention_probs_dropout_prog": 0.08691922263244126, "gradient_checkpointing": false, "hidden_act": {"act_type": "silu"}, "hidden_dropout_prob": 0.16232717925740514, "hidden_size": 430, "initializer_range": 0.02, "intermediate_size": 2864, "layer_norm_eps": 1e-12, "max_position_embeddings": 329, "num_attention_heads": 6, "num_hidden_layers": 3, "position_embedding_type": {"embedding_type": "relative_key_query"}, "type_vocab_size": 1, "use_cache": true, "vocab_size": 21494}